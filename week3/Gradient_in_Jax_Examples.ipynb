{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8e5lqwewuV4",
        "outputId": "4546781d-2aeb-4c76-bffd-898c56ae2c5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (0.4.26)\n",
            "Collecting jax\n",
            "  Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (0.4.26+cuda12.cudnn89)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.4.30-cp310-cp310-manylinux2014_x86_64.whl (79.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from jax) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax) (1.11.4)\n",
            "Installing collected packages: jaxlib, jax\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.26+cuda12.cudnn89\n",
            "    Uninstalling jaxlib-0.4.26+cuda12.cudnn89:\n",
            "      Successfully uninstalled jaxlib-0.4.26+cuda12.cudnn89\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.26\n",
            "    Uninstalling jax-0.4.26:\n",
            "      Successfully uninstalled jax-0.4.26\n",
            "Successfully installed jax-0.4.30 jaxlib-0.4.30\n"
          ]
        }
      ],
      "source": [
        "!pip install -U jax jaxlib # install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "import numpy as np\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "IUCIXjnqw7Jh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us consider the function $f(x) = 2x^2 + 3x - 4$. If we differentiate this function, we get $f^\\prime(x) = \\frac{df}{dx} = 4x + 3$."
      ],
      "metadata": {
        "id": "ToLmUVCuxJMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implement f from above\n",
        "def f(x):\n",
        "  return 2 * x * x + 3 * x + 4\n",
        "\n",
        "# implement derivative\n",
        "def f_prime_analytical(x):\n",
        "  return 4 * x + 3\n",
        "\n",
        "# we can also implement a function that computes the derivative numerically\n",
        "def f_prime_numerical(x, eps=0.0000001):\n",
        "  diff = f(x + eps) - f(x)\n",
        "  denom = eps\n",
        "  return diff / denom\n",
        "\n",
        "#using Jax, generate a function that computes the derivative of f instead\n",
        "f_prime_automatic = grad(f)"
      ],
      "metadata": {
        "id": "zccZHi-cxjJD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f_prime_analytical(10.0))\n",
        "print(f_prime_numerical(10.0))\n",
        "print(f_prime_automatic(10.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFoxlX6wygBm",
        "outputId": "4f225d58-2c5f-4605-98e4-b43b041bb448"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43.0\n",
            "43.000000005122274\n",
            "43.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from above, all three methods achieve more or less the same result, but using Jax to automatically generate a function to compute the derivative was the easiest approach. This makes it easy to implement gradient descent"
      ],
      "metadata": {
        "id": "w5Ve9wbiyuMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01 # set learning rate\n",
        "num_iterations = 1000 # set maximum number of iterations\n",
        "current_guess = np.random.random() # set initial guess\n",
        "print('Initial guess ', current_guess)\n",
        "for i in tqdm.tqdm(range(num_iterations)): #tqdm is used to help us monitor progress\n",
        "  current_gradient = f_prime_automatic(current_guess)\n",
        "  update = learning_rate * current_gradient\n",
        "  current_guess = current_guess - update\n",
        "\n",
        "print('')\n",
        "print('Minimum at ', current_guess)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emt2RJPmynx4",
        "outputId": "9a2656e0-70ec-427c-844e-bedae5dc6d6b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial guess  0.5967380372999491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:05<00:00, 172.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-0.7499993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also easily derive the gradients for multivariate functions. To illustrate, let us consider the function $f(x, y) = (x - 2)^2 + (y + 3)^2$. This function has a minimum at $\\left( 2, -3 \\right)$. Let us implement gradient descent to find this minimum"
      ],
      "metadata": {
        "id": "djA9rytb1gtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x, y):\n",
        "  return (x - 2) ** 2 + (y + 3) ** 2\n",
        "\n",
        "f_grad_x = grad(f, 0) # derivative wrt to the first argument, i.e. x. Remember we count from 0!\n",
        "f_grad_y = grad(f, 1) # derivative wrt to the second argument, i.e. y\n",
        "print(f_grad_x(2.0, -3.0))\n",
        "print(f_grad_y(2.0, -3.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wwvGkh-1pWM",
        "outputId": "729f9a41-d394-42bc-8407-7b950a26f9f0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1 # set learning rate\n",
        "num_iterations = 1000 # set maximum number of iterations\n",
        "x_guess = np.random.random() # set initial guess for x\n",
        "y_guess = np.random.random() # set initial guess for y\n",
        "print('Initial guess ', (x_guess, y_guess))\n",
        "for i in tqdm.tqdm(range(num_iterations)):\n",
        "  x_grad, y_grad = f_grad_x(x_guess, y_guess), f_grad_y(x_guess, y_guess)\n",
        "  x_guess = x_guess - learning_rate * x_grad\n",
        "  y_guess = y_guess - learning_rate * y_grad\n",
        "\n",
        "print()\n",
        "print(f'Minimum at {(float(x_guess), float(y_guess))}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YV3l_X_f0574",
        "outputId": "0293aa5c-a23a-4533-e06a-dbaab1de7c5b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial guess  (0.8675157920045229, 0.4547974086534248)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:08<00:00, 124.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Minimum at (1.999999761581421, -2.999999523162842)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an exercise, use gradient descent to minimize the following function:\n",
        "\n",
        "$$\n",
        "f(x, y) = 0.26(x^2 + y^2) - 0.48xy\n",
        "$$"
      ],
      "metadata": {
        "id": "ImqJNtf397yw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XXxgPW_2dDj",
        "outputId": "d5e286e9-6e11-49a1-f712-ab273a24901a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.6666666666666665"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xkhz_lmw7Rk5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}