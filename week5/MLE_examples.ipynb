{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62YccIAnbCB4",
        "outputId": "3a5a3960-f74a-46cc-a29c-e7a2d61f655d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (0.4.26)\n",
            "Collecting jax\n",
            "  Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (0.4.26+cuda12.cudnn89)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.4.30-cp310-cp310-manylinux2014_x86_64.whl (79.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from jax) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax) (1.11.4)\n",
            "Installing collected packages: jaxlib, jax\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.26+cuda12.cudnn89\n",
            "    Uninstalling jaxlib-0.4.26+cuda12.cudnn89:\n",
            "      Successfully uninstalled jaxlib-0.4.26+cuda12.cudnn89\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.26\n",
            "    Uninstalling jax-0.4.26:\n",
            "      Successfully uninstalled jax-0.4.26\n",
            "Successfully installed jax-0.4.30 jaxlib-0.4.30\n"
          ]
        }
      ],
      "source": [
        "!pip install -U jax jaxlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "import numpy as np\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "Ck7jueGpbFXr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting an Exponential Distribution\n",
        "\n",
        "In this example, we will consider fitting an exponential distribution on some randomly generated data and then observing the output. Recall that the PDF of an exponential distribution is\n",
        "$$\n",
        "  f(x; \\lambda) = λe^{-λx}\n",
        "$$\n",
        "\n",
        "Using this, we can code this PDF using the appropriate operations from Jax and use it in another function that computes the Negative Log-Liklihood for us to minimize using gradient descent. Note that the loss function must consume both the data and the parameters of the distribution, but that the paramter, in this case $λ$ **must** come first in the argument list since we want to take its gradient for optimization purposes"
      ],
      "metadata": {
        "id": "fTPnUhnIizRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf(lam, x):\n",
        "  return lam * jnp.exp(-lam * x)\n",
        "\n",
        "def loss_function(lam, x, eps=0.000000001):\n",
        "  components_before_log = pdf(lam, x)\n",
        "  # take log of each component. We add a small corrective factor to\n",
        "  #prevent numerical underflow in the log function\n",
        "  components_after_log = jnp.log(components_before_log + eps)\n",
        "  nll = -jnp.sum(components_after_log)\n",
        "  return nll"
      ],
      "metadata": {
        "id": "pKtuUFEIbabf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rate_parameter = 1.4  # You can adjust this value as needed\n",
        "\n",
        "# Sample 100 values from the exponential distribution\n",
        "sample_size = 100\n",
        "exponential_samples = np.random.exponential(1 / rate_parameter, sample_size)\n",
        "exponential_samples = jnp.array(exponential_samples)"
      ],
      "metadata": {
        "id": "D8yJeH5AcMGU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function_grad = grad(loss_function, 0)"
      ],
      "metadata": {
        "id": "yDJwDtLAcYW0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01 # set learning rate\n",
        "num_iterations = 5000 # set maximum number of iterations\n",
        "current_guess = np.random.random() # set initial guess\n",
        "print('Initial guess ', current_guess)\n",
        "for i in tqdm.tqdm(range(num_iterations)): #tqdm is used to help us monitor progress\n",
        "  current_gradient = loss_function_grad(current_guess, exponential_samples)\n",
        "  update = learning_rate * current_gradient\n",
        "  current_guess = current_guess - update\n",
        "print('Final guess ', current_guess)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIIFCx2Aciuq",
        "outputId": "508eaf82-8586-4a3c-f657-e1be34be5e7c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial guess  0.6556326151788402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [00:35<00:00, 139.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final guess  1.4509127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the above result is close to our value for lambda. Let us now consider a two-parameter case with the [Gamma distribution](https://en.wikipedia.org/wiki/Gamma_distribution)"
      ],
      "metadata": {
        "id": "smo0rwVk355F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf(k, theta, x):\n",
        "  factor = 1 / (jax.scipy.special.gamma(k) * jnp.power(theta, k))\n",
        "  return factor * (jnp.power(x, k - 1) * jnp.exp(-x/theta))\n",
        "\n",
        "def loss_function(k, theta, x, eps=0.000001):\n",
        "  components_before_log = pdf(k, theta, x)\n",
        "  components_after_log = jnp.log(components_before_log + eps)\n",
        "  nll = -jnp.sum(components_after_log)\n",
        "  return nll\n",
        "\n",
        "loss_function_grad_k = grad(loss_function, 0)\n",
        "loss_function_grad_theta = grad(loss_function, 1)"
      ],
      "metadata": {
        "id": "9YH9Zpafc0My"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 2.0  # Shape parameter\n",
        "theta = 2.0  # Scale parameter\n",
        "\n",
        "# Sample size\n",
        "sample_size = 1000\n",
        "\n",
        "# Sample from the Gamma distribution\n",
        "gamma_samples = np.random.gamma(k, theta, sample_size)\n",
        "gamma_samples = jnp.array(gamma_samples)"
      ],
      "metadata": {
        "id": "gWTy6PkDc4x_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001 # set learning rate\n",
        "num_iterations = 1000 # set maximum number of iterations\n",
        "k_guess = np.random.random() # set initial guess\n",
        "theta_guess = np.random.random() # set initial guess\n",
        "print('Initial guess ', (k_guess, theta_guess))\n",
        "for i in tqdm.tqdm(range(num_iterations)): #tqdm is used to help us monitor progress\n",
        "\n",
        "  k_grad = loss_function_grad_k(k_guess, theta_guess, gamma_samples)\n",
        "  theta_grad = loss_function_grad_theta(k_guess, theta_guess, gamma_samples)\n",
        "\n",
        "  k_guess = k_guess - learning_rate * k_grad\n",
        "  theta_guess = theta_guess - learning_rate * theta_grad\n",
        "\n",
        "print('\\nFinal guess ', (k_guess, theta_guess))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYgMbaKXfg6S",
        "outputId": "5b318767-1ce8-475b-982d-93e956209174"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial guess  (0.8046370082095061, 0.6335587655633199)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:29<00:00, 34.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final guess  (Array(1.9457594, dtype=float32, weak_type=True), Array(2.0648713, dtype=float32, weak_type=True))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make our code more well-abstracted and simplier, we can instead pass an array as our paramter, with one component containing the value for $k$ and another with our value for $\\theta$"
      ],
      "metadata": {
        "id": "osJLdvc24e-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf(params, x):\n",
        "  k = params[0]\n",
        "  theta = params[1]\n",
        "  factor = 1 / (jax.scipy.special.gamma(k) * jnp.power(theta, k))\n",
        "  return factor * (jnp.power(x, k - 1) * jnp.exp(-x/theta))\n",
        "\n",
        "def loss_function(params, x, eps=0.000001):\n",
        "  components_before_log = pdf(params, x)\n",
        "  components_after_log = jnp.log(components_before_log + eps)\n",
        "  nll = -jnp.sum(components_after_log)\n",
        "  return nll\n",
        "\n",
        "loss_function_grad_params = grad(loss_function, 0) # only need one gradient function"
      ],
      "metadata": {
        "id": "Ld7CUD-8gNi3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001 # set learning rate\n",
        "num_iterations = 1000 # set maximum number of iterations\n",
        "param_guess = np.random.random(size=2)\n",
        "print('Initial guess ', param_guess)\n",
        "for i in tqdm.tqdm(range(num_iterations)): #tqdm is used to help us monitor progress\n",
        "  param_grad = loss_function_grad_params(param_guess, gamma_samples)\n",
        "  param_guess = param_guess - learning_rate * param_grad # exploit broadcasting to simplify operation\n",
        "\n",
        "print('\\nFinal guess ', (k_guess, theta_guess))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS266uGp48fC",
        "outputId": "f1af1734-0a30-467b-c159-915246b9f006"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial guess  [0.70806647 0.78696594]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:19<00:00, 50.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final guess  (Array(1.9457594, dtype=float32, weak_type=True), Array(2.0648713, dtype=float32, weak_type=True))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this simplification of operations, we can abstract away our SGD loop into a function. We can also even abstract out our gradient function as an argument"
      ],
      "metadata": {
        "id": "ZR2vDNk05Yx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(param_guess, grad_func, samples, learning_rate=0.001, num_iterations=1000):\n",
        "  for i in tqdm.tqdm(range(num_iterations)): #tqdm is used to help us monitor progress\n",
        "    param_grad = grad_func(param_guess, samples)\n",
        "    param_guess = param_guess - learning_rate * param_grad # exploit broadcasting to simplify operation\n",
        "  return param_guess"
      ],
      "metadata": {
        "id": "Gfav-trk5PvO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the above, we can even go further and abstract the entire MLE pipeline by supplying a PDF, samples, and number of parameters!"
      ],
      "metadata": {
        "id": "JoQG6os_6COJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mle(num_params, pdf, samples, learning_rate=0.001, num_iterations=1000):\n",
        "  # create a function within a function\n",
        "  # see https://realpython.com/python-functional-programming/\n",
        "  def loss_function(params, x, eps=0.000001):\n",
        "    components_before_log = pdf(params, x)\n",
        "    components_after_log = jnp.log(components_before_log + eps)\n",
        "    nll = -jnp.sum(components_after_log)\n",
        "    return nll\n",
        "\n",
        "  loss_function_grad_params = grad(loss_function, 0)\n",
        "  param_guess = np.random.random(size=num_params)\n",
        "  param_guess = sgd(param_guess, loss_function_grad_params, samples,\n",
        "                    learning_rate=0.001, num_iterations=1000)\n",
        "  return param_guess"
      ],
      "metadata": {
        "id": "FbB24Kgj5_a0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exponential_pdf(lam, x):\n",
        "  return lam * jnp.exp(-lam * x)\n",
        "\n",
        "lambda_estimate = mle(num_params=1, pdf=exponential_pdf, samples=exponential_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vs6aQU560IX",
        "outputId": "a004cb38-cc8f-41f2-de85-ae7dde112bf2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:08<00:00, 117.37it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lambda_estimate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7XieY747Isv",
        "outputId": "2000024c-5cd8-45bd-f7d7-0485d63edbc7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.4510068]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gamma_pdf(params, x):\n",
        "  k = params[0]\n",
        "  theta = params[1]\n",
        "  factor = 1 / (jax.scipy.special.gamma(k) * jnp.power(theta, k))\n",
        "  return factor * (jnp.power(x, k - 1) * jnp.exp(-x/theta))\n",
        "\n",
        "param_estimates = mle(num_params=2, pdf=gamma_pdf, samples=gamma_samples)\n",
        "print('\\nEstimate of k ', param_estimates[0])\n",
        "print('Estimate of theta ', param_estimates[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyeOyqwl7g0Z",
        "outputId": "b561ad87-c557-4665-9b98-f4e43d5f1b71"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:17<00:00, 56.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Estimate of k  1.9457594\n",
            "Estimate of theta  2.0648715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-bQxoewY7u6a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}